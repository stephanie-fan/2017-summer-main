{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2: Word Embeddings\n",
        "\n",
        "In this part of the assignment, we'll explore word embeddings a little more deeply.\n",
        "\n",
        "If you haven't seen the [week3/embeddings.ipynb](../../../materials/week3/embeddings.ipynb) demo notebook, we recommend you look through it; this part of the assignment will build on that material."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This runs a shell command from the notebook.\n",
        "!pip install plotly\n",
        "\n",
        "# Standard python helper libraries.\n",
        "import os, sys, re, json, time\n",
        "import itertools, collections\n",
        "from IPython.display import display\n",
        "\n",
        "# NumPy and SciPy for matrix ops\n",
        "import numpy as np\n",
        "import scipy.sparse\n",
        "\n",
        "# utils.pretty_print_matrix uses Pandas. Configure float format here.\n",
        "import pandas as pd\n",
        "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
        "\n",
        "# NLTK for NLP utils\n",
        "import nltk\n",
        "\n",
        "# Helper libraries\n",
        "from shared_lib import utils, vocabulary, tf_embed_viz\n",
        "import part2_helpers\n",
        "\n",
        "# Plotly imports.\n",
        "import plotly.offline as plotly\n",
        "plotly.offline.init_notebook_mode()\n",
        "import plotly.graph_objs as go"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part (a): Nearest Neighbors\n",
        "\n",
        "In this part, we'll explore nearby words to get a feel for how our embeddings capture word similarity.\n",
        "\n",
        "We'll re-use the [week3/embeddings.ipynb](../../../materials/week3/embeddings.ipynb) code to build SVD-based embeddings from word-word co-occurrence counts. To keep this notebook brief, most of the code is included in `part2_helpers.py`, but we've glued it all together in the `embeddings_from_corpus` function below:\n",
        "\n",
        "- Load the corpus, process to tokens, and convert to ids.\n",
        "- Compute a word-word co-occurrence matrix $C_{ij}$ with a window of size $\\pm K$.\n",
        "- Apply the PPMI transformation to $C$.\n",
        "- Compute the SVD of $C$, truncated to $d$ dimensions.\n",
        "\n",
        "The end result is an embedding matrix $W$ of shape $|V| \\times d$, where each row $W_i$ is the $d$-dimensional representation of word $i$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def embeddings_from_corpus(corpus_name, K=1, d=100):\n",
        "    if corpus_name == \"reuters\":\n",
        "        assert(nltk.download('punkt'))\n",
        "    ##\n",
        "    # Load and pre-process the corpus\n",
        "    assert(nltk.download(corpus_name))  # make sure we have the data\n",
        "    corpus = utils.get_corpus(corpus_name)\n",
        "    vocab = vocabulary.Vocabulary(utils.canonicalize_word(w) for w in utils.flatten(corpus.sents()))\n",
        "    print \"Vocabulary: %d words\" % vocab.size\n",
        "\n",
        "    tokens = part2_helpers.sents_to_tokens(corpus.sents(), vocab)\n",
        "    print \"%g tokens\" % len(tokens)\n",
        "    token_ids = vocab.words_to_ids(tokens)\n",
        "    \n",
        "    ##\n",
        "    # Compute co-occurrence matrix and word vectors\n",
        "    t0 = time.time()\n",
        "    C = part2_helpers.cooccurrence_matrix(token_ids, vocab.size, K=K)\n",
        "    print \"Computed Co-occurrence matrix in %s\" % utils.pretty_timedelta(since=t0); t0 = time.time()\n",
        "    C_ppmi = part2_helpers.PPMI(C)\n",
        "    print \"Computed PPMI in %s\" % utils.pretty_timedelta(since=t0); t0 = time.time()\n",
        "    Wv, _ = part2_helpers.SVD(C_ppmi, d=d)\n",
        "    print \"Computed SVD in %s\" % utils.pretty_timedelta(since=t0)\n",
        "    \n",
        "    return Wv, vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use our code to build embeddings on the Brown corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Wv1_brown, vocab_brown = embeddings_from_corpus('brown', K=1, d=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NLTK includes wrappers for a number of corpora; the full list is available at http://www.nltk.org/nltk_data/\n",
        "\n",
        "For this assignment, we'll be using:\n",
        "- [The Brown corpus](http://www.hit.uib.no/icame/brown/bcm.html): `'brown'`\n",
        "- [The Reuters corpus](http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html): `'reuters'`\n",
        "- [Movie review data](http://www.cs.cornell.edu/people/pabo/movie-review-data/): `'movie_reviews'`\n",
        "\n",
        "These corpora are all similar in size, between 1-2 million words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For completeness' sake, we've also included the TensorBoard visualization code from the demo. This is totally optional - you don't need it at all for this section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Change these to visualize a different set\n",
        "Wv = Wv1_brown\n",
        "vocab = vocab_brown\n",
        "\n",
        "ev = tf_embed_viz.TFEmbeddingVizWrapper()\n",
        "ev.write_vocab_file(words=vocab.ids_to_words(range(Wv.shape[0])))\n",
        "ev.write_embeddings(Wv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cosine Similarity\n",
        "\n",
        "To measure the similarity of two words, we'll use the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) between their representation vectors:\n",
        "\n",
        "$$ D^{cos}_{ij} = \\frac{v_i^T v_j}{||v_i||\\ ||v_j||}$$\n",
        "\n",
        "*Note that this is called cosine similarity because $D^{cos}_{ij} = \\cos(\\theta_{ij})$, where $\\theta_{ij}$ is the angle between the two vectors.*\n",
        "\n",
        "### Part (a) tasks:\n",
        "\n",
        "For some of these tasks, you'll want to define new variables and/or make new code cells as intermediate steps. For grading, however, we're only going to look at the designated cells.\n",
        "\n",
        "1. Implement the `find_nn_cos` function in the cell below. Read the docstring _carefully_ - it describes what you should return. *Hint:* try to use NumPy functions (see below) instead of a `for` loop.\n",
        "2. Using the `embedding_from_corpus` function, generate 100-dimensional embeddings from the Brown corpus with $K = 1$, $K = 3$, and $K = 5$. Using the `show_nns` function, find the nearest neighbors for the word `\"washington\"` in each set of embeddings. *Briefly* describe how the set of nearest neighbors changes as you increase the window width $K$. Inlcude the nearest-neighbor lists in your answer.\n",
        "3. Generate 100-dimensional embeddings from the Reuters corpus (`'reuters'`) with $K = 3$. Find the nearest neighbors of the word `\"money\"` in these embeddings and in the $K=3$ embeddings from the Brown corpus. How are the sets of words different? Give a *brief* rationale for this based on your answer to the previous question and the differences between the corpora.\n",
        "4. Generate 100-dimensional embeddings from the Movie reviews corpus (`'movie_reviews'`) with $K = 3$. Compare the nearest neighbors of the word `\"great\"` against the $K = 3$ embeddings from Brown and Reuters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Answers to Part (a)\n",
        "\n",
        "**Put your answers to 2, 3, and 4 here.**\n",
        "\n",
        "1. Fill in code cell below.\n",
        "2. *Your answer here!*\n",
        "3. *Your answer here!*\n",
        "4. *Your answer here!*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "code_folding": [],
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "## GRADED_CELL: part_a_1\n",
        "def find_nn_cos(v, Wv, k=10):\n",
        "    \"\"\"Find nearest neighbors of a given word, by cosine similarity.\n",
        "    \n",
        "    Returns two parallel lists: indices of nearest neighbors, and \n",
        "    their cosine similarities. Both lists are in descending order, \n",
        "    and inclusive: so nns[0] should be the input word, nns[1] should be \n",
        "    the index of the first nearst neighbor, and so on.\n",
        "    \n",
        "    You may find the following numpy functions useful:\n",
        "      np.linalg.norm : take the l2-norm of a vector or matrix\n",
        "      np.dot : dot product or matrix multiplication\n",
        "      np.argsort : get indices sorted by element value,\n",
        "        so np.argsort(numbers)[-5:] will return the top five elements\n",
        "    \n",
        "    Args:\n",
        "      v: (d-dimensional vector) word vector of interest\n",
        "      Wv: (V x d matrix) word embeddings\n",
        "      k: (int) number of neighbors to return\n",
        "    \n",
        "    Returns (nns, ds), where:\n",
        "      nns: (k-element vector of ints), \n",
        "        row indices of nearest neighbors, including the given word\n",
        "      ds: (k-element vector of floats), \n",
        "        cosine similarity of each neighbor in nns\n",
        "    \"\"\"\n",
        "    pass\n",
        "    #### YOUR CODE HERE ####\n",
        "\n",
        "\n",
        "\n",
        "    #### END(YOUR CODE) ####\n",
        "    \n",
        "def show_nns(word, Wv, vocab, k=10):\n",
        "    \"\"\"Helper function to print neighbors of a given word.\"\"\"\n",
        "    word = utils.canonicalize_word(word, wordset=vocab.wordset)\n",
        "    print \"Nearest neighbors for \\\"%s\\\"\" % word\n",
        "    for i, d in zip(*find_nn_cos(Wv[vocab.word_to_id[word]], Wv, k)):\n",
        "        w = vocab.id_to_word[i]\n",
        "        print \"%.03f : \\\"%s\\\"\" % (d, w)\n",
        "    print \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In lieu of a whole separate test module for this function, we'll just give a sanity check here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Sanity check find_nn_cos\n",
        "inputs = (Wv[vocab_brown.word_to_id[\"washington\"]], Wv1_brown, 3)\n",
        "expected_nns = vocab_brown.words_to_ids([\"washington\", \"england\", \"georgia\"])\n",
        "expected_ds = [1.000, 0.714, 0.696]\n",
        "nns, ds = find_nn_cos(*inputs)\n",
        "\n",
        "ok = all(nns == expected_nns)\n",
        "print \"Neighbors match - ok.\" if ok else \"Neighbors don't match expected!\"\n",
        "\n",
        "ok = np.allclose(ds, [1.000, 0.714, 0.696], rtol=3e-3)\n",
        "print \"Distances match - ok.\" if ok else \"Distances don't match expected!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the `show_nns` function to print the nearest neighbors for a given word. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "show_nns(\"washington\", Wv1_brown, vocab_brown)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "## Part (a) 2.\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "## Part (a) 3.\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "## Part (a) 4.\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part (b): Bag-of-Vectors Classifier\n",
        "\n",
        "In Week 4, we'll apply our continuous word representations to the language modeling task. But word embeddings are useful for many other tasks as well!\n",
        "\n",
        "Here, we'll build a simple sentiment classifier using our SVD-based embeddings. The dataset is `movie_reviews`, aka [Polarity Dataset 2.0](http://www.cs.cornell.edu/people/pabo/movie-review-data/) from Cornell, which contains 1000 positive ($y = 1$) and 1000 negative ($y = 0$) movie reviews.\n",
        "\n",
        "Recall the simple bag-of-words logistic regression, which we might use as a baseline:\n",
        "\n",
        "$$\\hat{y} = \\sigma(\\sum_{x_{j} \\in x} W_j x_{j} + b) = \\sigma(W_{bow}x_{bow} + b)$$\n",
        "\n",
        "where $x_{j}$ is the number of times the word $w_j$ appears in the example (document) $x$, and our model parameters are a $|V|$-dimensional weight vector $W$ and a bias term $b$.\n",
        "\n",
        "If we have pre-trained word embeddings, a simple extension of this is a **bag-of-vectors** model. Starting from the bag-of-words representation, we'll take the embedding vector of each word, then represent the example $x$ as the sum (or average) of the $d$-dimensional word vectors $U$:\n",
        "\n",
        "$$ x_{bov} = \\sum_{x_j \\in x} U_j x_j $$ \n",
        "\n",
        "If we treat these as $d$-dimensional features, we can train our usual logistic regression model $\\hat{y} = \\sigma (W_{bov} x_{bov} + b) $.\n",
        "\n",
        "Let's build a bag-of-vectors model, and see how the word embeddings can help us generalize. We'll train our embeddings - which are *unsupervised* - on the whole corpus, then train our classifier on only a subset of it. This simulates the common case where we have a small amount of labeled data, but access to a huge quantity of unlabeled text.\n",
        "\n",
        "First, we'll start with the bag-of-words matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "corpus = nltk.corpus.movie_reviews\n",
        "vocab = vocabulary.Vocabulary(utils.canonicalize_word(w) \n",
        "                              for w in utils.flatten(corpus.sents()))\n",
        "print \"Vocabulary: %d words\" % vocab.size\n",
        "\n",
        "pos = corpus.paras(categories='pos')\n",
        "neg = corpus.paras(categories='neg')\n",
        "y = np.array([1]*len(pos) + [0]*len(neg), dtype=int)\n",
        "print \"%d positive examples\" % sum(y == 1)\n",
        "print \"%d negative examples\" % sum(y == 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For bag-of-words, we'll construct a sparse matrix X\n",
        "# Don't worry about how this code block works.\n",
        "ii, jj = [], []\n",
        "for i, para in enumerate(itertools.chain(pos, neg)):\n",
        "    tokens = part2_helpers.sents_to_tokens(para, vocab)\n",
        "    token_ids = vocab.words_to_ids(tokens)\n",
        "    ii.extend([i]*len(token_ids))  # row indices\n",
        "    jj.extend(token_ids)           # column indices (words)\n",
        "\n",
        "X_bow = scipy.sparse.csr_matrix((np.ones_like(ii), (ii, jj)), \n",
        "                                shape=[len(y),vocab.size])\n",
        "\n",
        "print (\"Bag-of-words matrix X_bow: %d x %d\" % X_bow.shape)\n",
        "print (\"  %g nonzero elements\" % X_bow.nnz)\n",
        "print (\"  %g total tokens\" % X_bow.sum())\n",
        "print (\"Label vector Y: %d elements\" % y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We've provided some helper code in `part2_helpers.py` to call Scikit-learn's logistic regression routines. It will run grid search with random cross-validation and return `(accuracy, accuracy_stdev)` on the left-out data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reload(part2_helpers)\n",
        "from part2_helpers import train_logistic_cv\n",
        "score, score_std = train_logistic_cv(X_bow, y, verbose=True)\n",
        "\n",
        "# You should get about 81% with the bag-of-words model\n",
        "assert(0.80 < score); assert(score < 0.85)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part (b) tasks:\n",
        "\n",
        "Per usual convention, the data matrix $X$ is the matrix where each row $x_i$ is the features for the $i^{th}$ train/test example.\n",
        "\n",
        "1. Given $n$ training examples, our bag-of-words data matrix $X_{bow}$ will be a sparse matrix of shape $n \\times |V|$. Write the shape of the bag-of-*vectors* data matrix $X_{bov}$, and give a matrix equation to compute $X_{bov}$ given the word embedding matrix $U$.\n",
        "2. Given your answer above, does using the word embeddings in a linear classifier add any expressive power to the model?\n",
        "3. In the cell below (under `#### YOUR CODE HERE ####`), compute $X_{bov}$. You should re-use the `embeddings_from_corpus` function from part (a) to compute 100-dimensional embeddings with $K=3$ on the `movie_reviews` corpus.\n",
        "4. Use the `train_logistic_cv` function to compare the performance of the bag-of-words model against the bag-of-vectors model, for different amounts of training data. (Note that your embeddings from 3. should be trained on the *entire* corpus.)\n",
        "5. Which model performs better in the small-dataset case? What if we use a larger fraction of the corpus? *Briefly* explain how the embeddings help your model, and how they can hold it back."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Answers to Part (b):\n",
        "\n",
        "Answer 1, 2, and 5 here. Please keep answers brief (1-2 lines).\n",
        "\n",
        "Hint: You can use LaTeX to typeset math, e.g. `$ f(x) = x^2 $` will render as $ f(x) = x^2 $.\n",
        "\n",
        "1. *Your answer here!*\n",
        "2. *Your answer here!*\n",
        "3. Fill in code cell below.\n",
        "4. Fill in code cell below.\n",
        "5. *Your answer here!*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "## GRADED_CELL: part_b_3\n",
        "#### YOUR CODE HERE ####\n",
        "\n",
        "\n",
        "#### END(YOUR CODE) ####\n",
        "print (\"Bag-of-vectors matrix X_bov: %d x %d\" % X_bov.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "## Use this cell for part_b_4\n",
        "# Will take a few minutes to run everything\n",
        "ns = np.array([10, 20, 50, 100, 200, 500, 1000])\n",
        "bow_scores = []  # scores, stds with bag-of-words model\n",
        "bov_scores = []  # scores, stds with bag-of-vectors model\n",
        "for N in ns:\n",
        "    print \"Running with N=%d\" % N\n",
        "    #### YOUR CODE HERE ####\n",
        "    \n",
        "    \n",
        "    #### END(YOUR CODE) ####\n",
        "bow_scores = np.array(bow_scores)\n",
        "bov_scores = np.array(bov_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Use this to plot the scores of the two models\n",
        "data = [go.Scatter(x = ns, y = bow_scores[:,0], name=\"Bag-of-Words\",\n",
        "                   mode='markers+lines', \n",
        "                   error_y=dict(type='data', array=bow_scores[:,1])),\n",
        "        go.Scatter(x = ns, y = bov_scores[:,0], name=\"Bag-of-Vectors\",\n",
        "                   mode='markers+lines',\n",
        "                   error_y=dict(type='data', array=bov_scores[:,1]))]\n",
        "layout = go.Layout(title=\"Dev accuracy by training examples\", xaxis=dict(type='log'))\n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "plotly.iplot(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [conda root]",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
